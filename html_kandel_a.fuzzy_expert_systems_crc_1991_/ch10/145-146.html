<HTML>
<HEAD>
<META name=vsisbn content="084934297x">
<META name=vstitle content="Fuzzy Expert Systems">
<META name=vsauthor content="Abraham Kandel">
<META name=vsimprint content="CRC Press">
<META name=vspublisher content="CRC Press LLC">
<META name=vspubdate content="11/01/91">
<META name=vscategory content="Web and Software Development: Artificial Intelligence: Fuzzy Logic">





<TITLE>Fuzzy Expert Systems:Fuzzy Associative Memory Systems</TITLE>

<!-- HEADER -->

<STYLE type="text/css"> 
 <!--
 A:hover  {
 	color : Red;
 }
 -->
</STYLE>

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
<script>
<!--
function displayWindow(url, width, height) {
         var Win = window.open(url,"displayWindow",'width=' + width +',height=' + height + ',resizable=1,scrollbars=yes');
	if (Win) {
		Win.focus();
	}
}
//-->
</script>
<SCRIPT>
<!--
function popUp(url) {
        var Win = window.open(url,"displayWindow",'width=400,height=300,resizable=1,scrollbars=yes');
	if (Win) {
		Win.focus();
	}
}
//-->
</SCRIPT>

<script language="JavaScript1.2">
<!--
function checkForQuery(fm) {
  /* get the query value */
  var i = escape(fm.query.value);
  if (i == "") {
      alert('Please enter a search word or phrase');
      return false;
  }                  /* query is blank, dont run the .jsp file */
  else return true;  /* execute the .jsp file */
}
//-->
</script>

</HEAD>

<BODY> 

<TABLE border=0 cellspacing=0 cellpadding=0>
<tr>
<td width=75 valign=top>
<img src="../084934297x.gif" width=60 height=73 alt="Fuzzy Expert Systems" border="1">
</td>
<td align="left">
    <font face="arial, helvetica" size="-1" color="#336633"><b>Fuzzy Expert Systems</b></font>
    <br>
    <font face="arial, helvetica" size="-1"><i>by Abraham Kandel</i>
    <br>
    CRC Press,&nbsp;CRC Press LLC
    <br>
    <b>ISBN:</b>&nbsp;084934297x<b>&nbsp;&nbsp;&nbsp;Pub Date:</b>&nbsp;11/01/91</font>&nbsp;&nbsp;
</td>
</tr>
</table>
<P>

<!--ISBN=084934297x//-->
<!--TITLE=Fuzzy Expert Systems//-->
<!--AUTHOR=Abraham Kandel//-->
<!--PUBLISHER=CRC Press LLC//-->
<!--IMPRINT=CRC Press//-->
<!--CHAPTER=10//-->
<!--PAGES=145-146//-->
<!--UNASSIGNED1//-->
<!--UNASSIGNED2//-->

<CENTER>
<TABLE BORDER>
<TR>
<TD><A HREF="143-145.html">Previous</A></TD>
<TD><A HREF="../ewtoc.html">Table of Contents</A></TD>
<TD><A HREF="146-148.html">Next</A></TD>
</TR>
</TABLE>
</CENTER>
<P><BR></P>
<P>which measure pointwise violations of subsethood m<SUB><SMALL>A</SMALL></SUB> &#8804; m<SUB><SMALL>B</SMALL></SUB> and supersethood m<SUB><SMALL>B</SMALL></SUB> &#8805; m<SUB><SMALL>A</SMALL></SUB>, as discussed in Section III.</P>
<P>We shall briefly illustrate fuzzy-entropy minimization by metric FAMs that are designed to minimize a Lyapunov or &#147;energy&#148; function. The neural network literature is too vast to survey here and the workings of all but the simplest nets are too lengthy to detail. The interested reader should consult the monumental works of Grossberg<SUP><SMALL>20-22</SMALL></SUP> and Kohonen<SUP><SMALL>32,33</SMALL></SUP> for the serious study of neural networks, brains, and minds as <I>programmable dynamic systems</I>.</P>
<P>The easiest hetero-/autoassociative metric FAM to study is Kohonen&#146;s optimal linear associative memory (OLAM).<SUP><SMALL>33</SMALL></SUP> The OLAM is not a dynamic associative memory. All recall takes place in one synchronous step. Therefore, the OLAM is not a fuzzy entropy minimizer. It is, however, optimal in the least-squares (1<SUP><SMALL>2</SMALL></SUP>) sense, and simple OLAMs can be constructed with pen and paper. We present it because it illustrates the properties of most metric associative memories and because it is the metric analogue of Theorems 1 and 2.</P>
<P>The OLAM stores m-many associations (A<SUB><SMALL>1</SMALL></SUB>,B<SUB><SMALL>1</SMALL></SUB>), . . . , (A<SUB><SMALL>m</SMALL></SUB>,B<SUB><SMALL>m</SMALL></SUB>) in an n-by-p matrix M. A<SUB><SMALL>i</SMALL></SUB> can be any real n-vector and B<SUB><SMALL>i</SMALL></SUB> any real p-vector, not necessarily points in I<SUP><SMALL>n</SMALL></SUP> and I<SUP><SMALL>p</SMALL></SUP>. B is recalled given A if B = AM. Then recall (decoding) is a linear procedure. We demand two things from M. First, A<SUB><SMALL>i</SMALL></SUB> M = B<SUB><SMALL>i</SMALL></SUB> must hold for all i; A<SUB><SMALL>i</SMALL></SUB> must perfectly recall its associated output B<SUB><SMALL>i</SMALL></SUB>. Second, if some input A is closer to A<SUB><SMALL>i</SMALL></SUB> than to all other A<SUB><SMALL>j</SMALL></SUB>, then the output B = AM must be closer to B<SUB><SMALL>i</SMALL></SUB> than to all other B<SUB><SMALL>j</SMALL></SUB>. We seek a parallel distributed construction (encoding) procedure for such an optimal M.</P>
<P>To simplify notation, we rewrite the recall equation in matrix notation as B = AM. Here A<SUP><SMALL>T</SMALL></SUP> = [A<SUB><SMALL>1</SMALL></SUB><SUP><SMALL>T</SMALL></SUP>| . . . |A<SUB><SMALL>m</SMALL></SUB><SUP><SMALL>T</SMALL></SUP>] and B<SUP><SMALL>T</SMALL></SUP> = [B<SUB><SMALL>1</SMALL></SUB><SUP><SMALL>T</SMALL></SUP>| . . . [B<SUB><SMALL>m</SMALL></SUB><SUP><SMALL>T</SMALL></SUP>] are rectangular matrices. A<SUB><SMALL>i</SMALL></SUB> is the i<SUP><SMALL>th</SMALL></SUP> row of the m-by-n matrix A. B<SUB><SMALL>i</SMALL></SUB> is the i<SUP><SMALL>th</SMALL></SUP> row of the m-by-p matrix B. Norm(M) is the matrix norm of M compatible with the vector 1<SUP><SMALL>2</SMALL></SUP> (Euclidean) norm, the square root of Trace(M<SUP><SMALL>T</SMALL></SUP>M). We search the space of n-by-p real matrices to find that matrix M that minimizes Norm(B - AM). In the special case where n = m = p and A<SUP><SMALL>-1</SMALL></SUP>, the matrix inverse of A, exists, the optimal M is simply A<SUP><SMALL>-1</SMALL></SUP>B. In general, the solution is M = A*B, where A* is the Moore-Penrose pseudo-inverse of A. The bidirectional<SUP><SMALL>37</SMALL></SUP> OLAM for recalling A given B is M* = B*A. The OLAM autoassociation matrix is M = A*A. A* uniquely exists for every matrix and a variety of recursive procedures exist to calculate it. The popular Greville&#146;s algorithm for computing A* enjoys an equivalence to the time-independent Kalman filter,<SUP><SMALL>30</SMALL></SUP> which underlies the practical power of the OLAM.</P>
<P>The <I>memory capacity</I> of the OLAM M is m &lt; n. No more associations can be stored in M and reliably recalled than the dimensionality of the pattern space &#151; a result true for most associative memories.<SUP><SMALL>1</SMALL></SUP> Kohonen<SUP><SMALL>33</SMALL></SUP> has shown that the OLAM attenuates/amplifies circularly symmetric noise according to <IMG SRC="images/10-21i.jpg">, again reflecting the capacity bound. The autoassociative OLAM A*A is an optimal projection operator. The OLAM uniquely decomposes any pattern P into signal and noise P<SUB><SMALL>s</SMALL></SUB> + P<SUB><SMALL>n</SMALL></SUB> in a Hilbert-space version of Pythagoras&#146;s theorem. P is the hypotenuse, P<SUB><SMALL>s</SMALL></SUB> and P<SUB><SMALL>n</SMALL></SUB> are the orthogonal legs. A*A projects P onto the subspace spanned by the patterns in A. The projection P<SUB><SMALL>s</SMALL></SUB> = P A*A is the best signal estimate (best &#147;prediction&#148;) of P given A. P<SUB><SMALL>n</SMALL></SUB> = P - P<SUB><SMALL>s</SMALL></SUB> is the orthogonal <I>novelty</I> (&#147;error&#148;) in P. I - A*A is a <I>novelty filter</I>, where I is the n-by-n identity matrix. It optimally measures what is new or unexplained in P with respect to what is known (stored in A). These properties rest on the properties of Fourier coefficients in Hilbert space and are thus structurally analogous to linear regression properties of uniquely decomposing a data vector into a best prediction vector and residual or error vector. Some neural net critics have even claimed that associative memory theory is disguised regression. The fundamental difference is that regression models assume a functional <I>dependency</I> between A and B, namely, B = cA + e, and attribute all disparities between B and cA to some stochastic error e. On the contrary, in associative memory theory we assume no dependencies between the arbitrary data vectors or sets A<SUB><SMALL>i</SMALL></SUB> and B<SUB><SMALL>i</SMALL></SUB>. We instead seek encoding structures, often matrices, that form an association between them in some appropriate way.</P><P><BR></P>
<CENTER>
<TABLE BORDER>
<TR>
<TD><A HREF="143-145.html">Previous</A></TD>
<TD><A HREF="../ewtoc.html">Table of Contents</A></TD>
<TD><A HREF="146-148.html">Next</A></TD>
</TR>
</TABLE>
</CENTER>

<hr width="90%" size="1" noshade>
<div align="center">
<font face="Verdana,sans-serif" size="1">Copyright &copy; <a href="/reference/crc00001.html">CRC Press LLC</a></font>
</div>
</BODY>

